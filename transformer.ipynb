{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math \n",
    "from math import sqrt \n",
    "# from layers.Transformer_EncDec import Decoder, DecoderLayer, Encoder, EncoderLayer\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.downConv = nn.Conv1d(in_channels=c_in,\n",
    "                                  out_channels=c_in,\n",
    "                                  kernel_size=3,\n",
    "                                  padding=2,\n",
    "                                  padding_mode='circular')\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x.permute(0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x + y), attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
    "                 dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask\n",
    "        )[0])\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask\n",
    "        )[0])\n",
    "\n",
    "        y = x = self.norm2(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm3(x + y)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers, norm_layer=None, projection=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "\n",
    "def compared_version(ver1, ver2):\n",
    "    \"\"\"\n",
    "    :param ver1\n",
    "    :param ver2\n",
    "    :return: ver1< = >ver2 False/True\n",
    "    \"\"\"\n",
    "    list1 = str(ver1).split(\".\")\n",
    "    list2 = str(ver2).split(\".\")\n",
    "    \n",
    "    for i in range(len(list1)) if len(list1) < len(list2) else range(len(list2)):\n",
    "        if int(list1[i]) == int(list2[i]):\n",
    "            pass\n",
    "        elif int(list1[i]) < int(list2[i]):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    if len(list1) == len(list2):\n",
    "        return True\n",
    "    elif len(list1) < len(list2):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if compared_version(torch.__version__, '1.5.0') else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "    \n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "    \n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "    \n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "    \n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "    \n",
    "\n",
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(FullAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1. / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), A)\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla Transformer with O(L^2) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.output_attention = configs.output_attention\n",
    "\n",
    "        # Embedding\n",
    "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                           configs.dropout)\n",
    "        self.dec_embedding = DataEmbedding(configs.dec_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                           configs.dropout)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,\n",
    "                                      output_attention=configs.output_attention), configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation\n",
    "                ) for l in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(configs.d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(True, configs.factor, attention_dropout=configs.dropout, output_attention=False),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs.factor, attention_dropout=configs.dropout, output_attention=False),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                )\n",
    "                for l in range(configs.d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(configs.d_model),\n",
    "            projection=nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import os\n",
    "# import torch\n",
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "# fix_seed = 2021\n",
    "# random.seed(fix_seed)\n",
    "# torch.manual_seed(fix_seed)\n",
    "# np.random.seed(fix_seed)\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# # basic config\n",
    "# parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "# parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "# parser.add_argument('--model', type=str, required=True, default='Transformer',\n",
    "#                     help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# # data loader\n",
    "# parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "# parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "# parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "# parser.add_argument('--features', type=str, default='M',\n",
    "#                     help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "# parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "# parser.add_argument('--freq', type=str, default='h',\n",
    "#                     help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "# parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# # forecasting task\n",
    "# parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "# parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "# parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "# # model define\n",
    "# parser.add_argument('--bucket_size', type=int, default=4, help='for Reformer')\n",
    "# parser.add_argument('--n_hashes', type=int, default=4, help='for Reformer')\n",
    "# parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')\n",
    "# parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "# parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "# parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "# parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "# parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "# parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "# parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "# parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "# parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "# parser.add_argument('--distil', action='store_false',\n",
    "#                     help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "#                     default=True)\n",
    "# parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "# parser.add_argument('--embed', type=str, default='timeF',\n",
    "#                     help='time features encoding, options:[timeF, fixed, learned]')\n",
    "# parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "# parser.add_argument('--output_attention', action='store_true', help='whether to output attention in encoder')\n",
    "# parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# # optimization\n",
    "# parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "# parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "# parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "# parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "# parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "# parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "# parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "# parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "# parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "# parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# # GPU\n",
    "# parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "# parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "# parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "# parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "# if args.use_gpu and args.use_multi_gpu:\n",
    "#     args.devices = args.devices.replace(' ', '')\n",
    "#     device_ids = args.devices.split(',')\n",
    "#     args.device_ids = [int(id_) for id_ in device_ids]\n",
    "#     args.gpu = args.device_ids[0]\n",
    "\n",
    "# print('Args in experiment:')\n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alice\n",
      "Age: 25\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# 创建 Namespace 并手动设置参数\n",
    "args = argparse.Namespace()\n",
    "setattr(args, \"name\", \"Alice\")\n",
    "setattr(args, \"age\", 25)\n",
    "\n",
    "# 输出参数\n",
    "print(f\"Name: {args.name}\")\n",
    "print(f\"Age: {args.age}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(is_training=1, model_id='test', model='Transformer', data='ETTm1', root_path='./data/ETT/', data_path='ETTh1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=96, label_len=48, pred_len=96, bucket_size=4, n_hashes=4, enc_in=7, dec_in=7, c_out=7, d_model=512, n_heads=8, e_layers=4, d_layers=4, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=True, do_predict=True, num_workers=10, itr=2, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='test', loss='mse', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "args = argparse.Namespace()\n",
    "\n",
    "# basic config\n",
    "setattr(args, 'is_training',1)\n",
    "setattr(args, 'model_id','test')\n",
    "setattr(args, 'model','Transformer')\n",
    "\n",
    "# data loader\n",
    "setattr(args, 'data','ETTm1')\n",
    "setattr(args, 'root_path','./data/ETT/')\n",
    "setattr(args, 'data_path','ETTh1.csv')\n",
    "setattr(args, 'features','M')\n",
    "setattr(args, 'target','OT')\n",
    "setattr(args, 'freq','h')\n",
    "setattr(args, 'checkpoints','./checkpoints/')\n",
    "\n",
    "# forecasting task\n",
    "setattr(args, 'seq_len',96)\n",
    "setattr(args, 'label_len',48)\n",
    "setattr(args, 'pred_len',96)\n",
    "\n",
    "# model define\n",
    "setattr(args, 'bucket_size',4)\n",
    "setattr(args, 'n_hashes',4)\n",
    "setattr(args, 'enc_in',7)\n",
    "setattr(args, 'dec_in',7)\n",
    "setattr(args, 'c_out',7)\n",
    "setattr(args, 'd_model',512)\n",
    "setattr(args, 'n_heads',8)\n",
    "setattr(args, 'e_layers',4)\n",
    "setattr(args, 'd_layers',4)\n",
    "setattr(args, 'd_ff',2048)\n",
    "setattr(args, 'moving_avg',25)\n",
    "setattr(args, 'factor',1)\n",
    "setattr(args, 'distil',True)\n",
    "setattr(args, 'dropout',0.05)\n",
    "setattr(args, 'embed','timeF')\n",
    "setattr(args, 'activation','gelu')\n",
    "setattr(args, 'output_attention',True)\n",
    "setattr(args, 'do_predict',True)\n",
    "\n",
    "# optimization\n",
    "setattr(args, 'num_workers',10)\n",
    "setattr(args, 'itr',2)\n",
    "setattr(args, 'train_epochs',10)\n",
    "setattr(args, 'batch_size',32)\n",
    "setattr(args, 'patience',3)\n",
    "setattr(args, 'learning_rate',0.0001)\n",
    "setattr(args, 'des','test')\n",
    "setattr(args, 'loss','mse')\n",
    "setattr(args, 'lradj','type1')\n",
    "setattr(args, 'use_amp',False)\n",
    "\n",
    "# GPU\n",
    "setattr(args, 'use_gpu',True)\n",
    "setattr(args, 'gpu',0)\n",
    "setattr(args, 'use_multi_gpu',False)\n",
    "setattr(args, 'devices','0,1,2,3')\n",
    "\n",
    "\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10752\n",
      "2048\n",
      "10752\n",
      "2048\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "1048576\n",
      "2048\n",
      "1048576\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "1048576\n",
      "2048\n",
      "1048576\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "1048576\n",
      "2048\n",
      "1048576\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "1048576\n",
      "2048\n",
      "1048576\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "1048576\n",
      "2048\n",
      "1048576\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "1048576\n",
      "2048\n",
      "1048576\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "1048576\n",
      "2048\n",
      "1048576\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "262144\n",
      "512\n",
      "1048576\n",
      "2048\n",
      "1048576\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "3584\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for para in model.parameters():\n",
    "    print(para.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (enc_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(7, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=4, out_features=512, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (dec_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(7, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TimeFeatureEmbedding(\n",
       "      (embed): Linear(in_features=4, out_features=512, bias=False)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (attn_layers): ModuleList(\n",
       "      (0-3): 4 x EncoderLayer(\n",
       "        (attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 10, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "\n",
    "a = np.random.normal(0,10,size=(16,10,3))\n",
    "a.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3185e+00,  1.7238e+01,  5.1672e+00],\n",
       "         [ 4.1623e+00, -5.4389e+00, -9.7597e+00],\n",
       "         [ 4.8039e+00, -1.8568e+01, -4.8590e+00],\n",
       "         [-1.2191e+00,  6.1390e+00,  3.8038e+00],\n",
       "         [ 6.3508e+00,  8.2808e+00,  6.5831e+00],\n",
       "         [ 1.4374e+01, -7.2608e+00,  1.6505e+01],\n",
       "         [-8.5949e+00, -1.2486e+01,  1.2401e+01],\n",
       "         [ 3.3475e+00, -1.9034e+00,  5.0182e+00],\n",
       "         [ 9.6723e+00,  6.6457e-01,  2.8736e+01],\n",
       "         [ 5.0771e+00, -2.2431e+01,  6.8319e+00]],\n",
       "\n",
       "        [[ 1.1060e+01,  1.2759e-01, -1.6435e+01],\n",
       "         [ 5.6412e+00, -6.0110e+00,  3.9698e+00],\n",
       "         [-2.8981e+00, -3.9642e+00, -1.3636e+01],\n",
       "         [-1.8569e+01, -5.2758e+00,  1.6356e+01],\n",
       "         [ 5.6917e+00,  6.5149e+00, -5.8526e+00],\n",
       "         [ 2.6840e+00,  3.2473e+00, -8.4002e+00],\n",
       "         [-1.4625e+01, -7.1586e+00, -7.3438e+00],\n",
       "         [-1.5875e+01, -8.5731e+00,  5.8538e+00],\n",
       "         [-6.8722e+00, -8.4711e+00, -4.7986e+00],\n",
       "         [-1.4864e+01, -3.2924e+00,  1.6070e+01]],\n",
       "\n",
       "        [[-9.7194e+00,  8.2918e+00,  2.1393e+00],\n",
       "         [-2.5499e+00, -2.6066e+00, -5.9510e+00],\n",
       "         [-7.3496e-01, -1.2015e+00,  9.9758e+00],\n",
       "         [ 3.0109e+00,  8.9806e+00,  9.0229e+00],\n",
       "         [ 7.1040e+00,  4.5365e-01, -2.0398e+01],\n",
       "         [-5.2245e+00,  1.0229e+01,  7.2657e-02],\n",
       "         [-1.3955e+01, -1.3320e+01,  1.6577e+00],\n",
       "         [-1.4730e+01,  3.0166e+00, -2.4688e+00],\n",
       "         [ 8.3578e+00,  8.8568e-02,  2.8679e+00],\n",
       "         [ 2.2502e+00, -9.2932e+00, -6.9241e+00]],\n",
       "\n",
       "        [[-7.2763e-02,  1.2368e+01,  2.3365e+00],\n",
       "         [-1.1749e+01,  7.4511e+00, -2.1776e+00],\n",
       "         [ 1.6921e+01, -5.5521e+00,  3.4550e+00],\n",
       "         [ 8.0822e+00,  1.7277e+00,  1.1755e+01],\n",
       "         [-5.6542e+00, -7.1581e-01, -5.4954e+00],\n",
       "         [-2.7357e+00,  1.7485e+00,  2.6471e+00],\n",
       "         [ 5.7993e+00,  7.2136e+00,  1.1235e+01],\n",
       "         [-7.4222e+00, -1.0465e+01,  1.3071e+01],\n",
       "         [ 1.5545e+01, -1.0233e+01,  1.0644e+01],\n",
       "         [-2.3539e+01,  6.6870e+00, -1.3474e+01]],\n",
       "\n",
       "        [[-1.3055e+01,  1.9319e+01,  7.5345e-01],\n",
       "         [ 2.1415e-01, -9.4656e-01,  7.1710e+00],\n",
       "         [-5.1946e+00, -5.0373e+00, -6.9871e+00],\n",
       "         [-2.2042e+00,  2.3913e+00,  1.6742e+01],\n",
       "         [-7.2300e+00,  2.1608e+00, -2.5148e+00],\n",
       "         [ 2.2626e+00,  3.0127e+00, -8.9977e-01],\n",
       "         [ 2.5193e+01, -1.5182e+01,  6.0850e+00],\n",
       "         [ 1.3486e+01, -4.4588e+00,  1.1910e+01],\n",
       "         [ 1.8134e+00, -4.7348e+00, -2.1699e+01],\n",
       "         [-1.3180e+00, -1.0942e+01, -7.1499e+00]],\n",
       "\n",
       "        [[ 3.2721e+00,  1.3285e-01,  2.8343e+00],\n",
       "         [ 4.3621e+00, -1.7963e+00, -6.2780e+00],\n",
       "         [-2.1866e+01,  3.0775e+00,  9.5875e-02],\n",
       "         [ 4.9440e+00,  9.6140e+00,  3.5870e+00],\n",
       "         [ 1.4835e+01,  2.9390e+00,  4.9004e+00],\n",
       "         [ 2.7709e+00,  1.3673e+01, -4.5425e+00],\n",
       "         [ 1.6088e+00, -1.4350e+01,  6.2801e+00],\n",
       "         [-4.7152e+00,  2.4917e+01, -7.7321e+00],\n",
       "         [ 1.1420e+01,  6.4168e-01, -4.2485e+00],\n",
       "         [ 2.6059e+01,  3.7760e+00,  8.9366e+00]],\n",
       "\n",
       "        [[ 1.1830e+01,  5.9275e+00, -7.6278e-01],\n",
       "         [-2.1811e+00,  3.3366e+00, -3.5644e+00],\n",
       "         [-1.2402e+00, -8.7799e+00,  1.2320e+01],\n",
       "         [ 1.5654e+01, -1.8168e+01,  4.2506e+00],\n",
       "         [-1.0142e+01,  1.0489e+01, -7.7448e+00],\n",
       "         [ 2.0189e+00,  5.5417e+00,  1.3262e+00],\n",
       "         [-6.9690e+00, -1.5246e+01,  3.8094e+00],\n",
       "         [-9.4453e+00,  8.4922e+00, -2.6355e+00],\n",
       "         [ 2.9361e+00, -7.8588e+00,  1.5172e+01],\n",
       "         [-1.4292e+01,  1.4874e+01,  1.0642e+01]],\n",
       "\n",
       "        [[ 1.0370e+01, -2.9497e-01,  2.8136e+00],\n",
       "         [ 6.2270e+00, -4.8106e-01, -6.2057e+00],\n",
       "         [-1.0538e+01,  1.2058e+00,  2.0895e+00],\n",
       "         [-1.3463e+00,  8.5541e+00,  5.8364e+00],\n",
       "         [ 8.8192e+00,  1.0763e+01,  2.8898e+00],\n",
       "         [ 1.3363e+01,  5.6048e+00,  4.5185e+00],\n",
       "         [ 4.3327e+00,  9.1125e+00, -1.0662e+01],\n",
       "         [-1.1513e+01,  1.3226e+01,  2.1184e+00],\n",
       "         [-1.0511e+00,  2.4041e+00,  2.8481e+00],\n",
       "         [-1.3040e+00,  4.8864e+00, -1.1432e+01]],\n",
       "\n",
       "        [[ 2.4916e+00, -3.4664e+01,  7.4512e+00],\n",
       "         [-2.1454e+01,  9.2273e+00, -2.1285e+01],\n",
       "         [-3.4292e+00, -1.2095e+01, -1.4504e+01],\n",
       "         [ 3.0985e+00,  2.1243e+00, -9.6787e+00],\n",
       "         [ 5.7141e+00,  2.0974e+01,  1.1258e+01],\n",
       "         [ 2.4008e+01,  3.0970e+00, -7.5792e+00],\n",
       "         [ 1.4646e+01, -1.1316e+01, -5.3993e+00],\n",
       "         [-6.1008e+00,  1.5554e+01,  3.5419e+00],\n",
       "         [ 3.9017e+00,  6.8199e+00, -4.8096e-01],\n",
       "         [ 2.8949e+00,  1.5947e+00, -2.2115e+00]],\n",
       "\n",
       "        [[ 1.6045e+01, -4.3778e+00,  2.6056e+00],\n",
       "         [-4.1312e-01, -8.0848e+00, -1.2362e+01],\n",
       "         [ 1.5364e+00, -2.2475e+01, -1.3906e+00],\n",
       "         [ 2.0931e+00,  1.6876e+00,  7.4539e+00],\n",
       "         [ 6.6417e+00, -4.6032e+00, -5.3538e+00],\n",
       "         [-4.8122e+00,  7.2860e+00, -2.0199e+01],\n",
       "         [-3.6039e+00,  9.6774e+00,  6.7448e+00],\n",
       "         [-6.7215e+00, -1.7026e-01, -1.4157e+01],\n",
       "         [ 1.3390e+01,  8.6957e+00,  1.9003e+01],\n",
       "         [-2.2588e+00, -5.7341e+00,  3.8864e+00]],\n",
       "\n",
       "        [[-1.0732e+01,  3.3815e+00,  1.5065e+01],\n",
       "         [ 5.1209e+00, -1.4888e+01, -1.5718e+00],\n",
       "         [ 5.1818e+00,  1.1689e+01,  5.6510e+00],\n",
       "         [ 4.6475e-01,  1.1185e+01,  2.5984e+00],\n",
       "         [ 2.3305e+00,  1.3141e+01,  2.7503e-01],\n",
       "         [-1.1968e-01,  4.4491e+00,  4.3330e+00],\n",
       "         [-1.3368e+01, -4.5774e+00, -1.1820e+01],\n",
       "         [ 3.8824e+00, -2.4233e+00,  2.0004e+00],\n",
       "         [-1.1008e+00,  8.8993e+00, -1.8790e+01],\n",
       "         [-9.8142e-01,  1.8425e+00,  1.2880e+01]],\n",
       "\n",
       "        [[-7.2029e+00,  2.3292e+01, -9.3860e+00],\n",
       "         [-8.4035e+00,  1.7138e+01, -1.4742e+01],\n",
       "         [ 9.7986e+00,  1.2740e+01,  6.2221e+00],\n",
       "         [ 4.1091e+00,  6.3393e+00, -1.7013e+01],\n",
       "         [-3.7849e+00,  1.4762e+01, -1.5733e+00],\n",
       "         [ 2.0655e-03,  3.1142e+00,  1.3179e+01],\n",
       "         [ 1.8363e+01, -6.4542e+00, -6.9754e+00],\n",
       "         [ 8.5333e+00, -1.3699e+00, -4.5382e+00],\n",
       "         [-4.5701e+00,  1.3777e+01,  1.7126e-01],\n",
       "         [-8.9857e+00,  1.3066e+01,  8.4611e+00]],\n",
       "\n",
       "        [[ 1.4685e+01,  5.7988e+00, -1.6875e+01],\n",
       "         [-1.1545e+01, -1.7102e+00,  1.3542e+01],\n",
       "         [ 5.1009e+00, -7.4304e+00, -3.2380e+00],\n",
       "         [-9.7675e+00,  5.9839e+00,  2.9831e+00],\n",
       "         [-1.0875e+01,  8.6318e+00,  3.8204e+00],\n",
       "         [ 7.2625e-01, -1.1368e+01,  8.0119e+00],\n",
       "         [ 9.4172e+00, -3.2826e+00,  1.4541e+00],\n",
       "         [ 1.9761e+01,  1.1169e+01, -7.7952e+00],\n",
       "         [-2.0193e-01,  6.6626e+00, -1.5383e+01],\n",
       "         [-6.6282e+00,  1.6825e-01, -7.1780e+00]],\n",
       "\n",
       "        [[ 4.8467e+00, -8.2781e-01, -8.8679e-01],\n",
       "         [ 1.8655e+00,  4.2558e+00,  8.3439e+00],\n",
       "         [ 1.9219e+00, -1.2371e+01, -4.4930e-01],\n",
       "         [-1.8941e+00, -9.8013e+00,  1.1765e+01],\n",
       "         [-6.5728e+00, -7.4991e+00,  3.8499e+00],\n",
       "         [ 8.5110e+00, -1.1459e+01,  3.3798e+00],\n",
       "         [ 2.1621e+00,  1.2804e+01,  9.6591e-01],\n",
       "         [-1.4333e+01, -8.1651e+00,  1.0263e+01],\n",
       "         [-1.1820e+00, -2.3636e+01,  2.5253e+00],\n",
       "         [ 9.6078e-01,  1.7603e+01, -6.1450e+00]],\n",
       "\n",
       "        [[-2.5090e+01, -4.3490e+00, -3.0748e+00],\n",
       "         [-4.7892e+00, -3.4893e+00,  1.2040e+01],\n",
       "         [-7.9808e+00,  9.5508e-01,  4.2585e+00],\n",
       "         [ 2.0177e+01, -7.9080e+00, -2.4293e+00],\n",
       "         [-8.6698e+00, -1.4038e+01,  9.3266e+00],\n",
       "         [-1.3537e+01, -6.0087e+00, -8.5537e+00],\n",
       "         [ 6.6591e+00, -2.0412e+00,  1.5334e+00],\n",
       "         [ 7.8417e+00, -1.5798e+01,  3.0093e+00],\n",
       "         [ 1.3933e+01, -1.0560e+00, -6.0272e+00],\n",
       "         [ 9.5991e+00,  9.4683e+00, -6.4846e+00]],\n",
       "\n",
       "        [[ 6.0517e+00, -6.1057e+00, -2.4982e+01],\n",
       "         [ 1.7583e+01,  2.3990e+00, -3.8071e+00],\n",
       "         [-1.9387e+00,  1.0829e+01, -6.8957e+00],\n",
       "         [ 1.8790e+01,  1.8255e+00,  4.9106e+00],\n",
       "         [-1.3208e+01,  3.0145e+01, -9.9860e+00],\n",
       "         [-1.2662e+01,  1.1096e+01, -1.0463e+01],\n",
       "         [ 1.5425e+00,  2.6638e+00, -1.2203e+01],\n",
       "         [ 2.5448e+00,  3.1857e+00,  1.8781e+00],\n",
       "         [-7.0111e+00,  1.2012e+01, -7.3164e+00],\n",
       "         [-3.9871e+00, -1.5627e+01, -1.2492e+01]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.from_numpy(a)\n",
    "aa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设输入张量形状为 (16, 10, 3)\n",
    "input_tensor = torch.randn(16, 10, 3)\n",
    "\n",
    "# 使用全局平均池化后接全连接层\n",
    "x = input_tensor.permute(0, 2, 1)\n",
    "pool = nn.AdaptiveAvgPool1d(1)\n",
    "fc = nn.Linear(3, 1)  # 将3个特征映射到1个特征\n",
    "\n",
    "output = fc(pool(x).squeeze(-1))\n",
    "\n",
    "print(output.shape)  # 输出: torch.Size([16, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.permute(0,2,1).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
