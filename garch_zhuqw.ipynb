{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "df = pd.read_csv('./data/df.csv',sep=',')\n",
    "df  = df.sort_values(by='time',ascending=True).reset_index(drop=True)\n",
    "\n",
    "BATCHSIZE = 512\n",
    "LOOKBACK = 5\n",
    "\n",
    "\n",
    "date = [\"2024-02-19\",\n",
    "\"2024-03-15\",\n",
    "\"2024-04-19\",\n",
    "\"2024-05-17\",\n",
    "\"2024-06-21\",\n",
    "\"2024-07-19\",\n",
    "\"2024-08-16\",\n",
    "\"2024-09-20\",\n",
    "\"2024-10-18\",\n",
    "\"2024-11-15\",\n",
    "\"2024-12-20\",\n",
    "\"2025-01-17\"]\n",
    "df.head()\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['date'] = df['time'].dt.date\n",
    "df['date'] = df['date'].astype(str)\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['minute'] = df['time'].dt.minute \n",
    "# for i in range(1,5):\n",
    "#     df[f'spread_shift_{i}'] = df.groupby(['hour','minute'])['spread'].shift(i)\n",
    "\n",
    "df['Expiration_Date'] = df['date'].apply(lambda x: 1 if x in date else 0)\n",
    "# df.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yfinance\n",
    "# !pip install pandas-datareader\n",
    "\n",
    "# Import yfinance to make Yahoo Finance API call \n",
    "# import yfinance as yf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from functools import reduce \n",
    "\n",
    "# Import data reader to directly convert Yahoo Finance data into dataframe\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "# !pip install arch\n",
    "from arch import arch_model\n",
    "\n",
    "# ACF plot of standardized residuals\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Ljunb-Box test of standardized residuals\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Set seaborn plot style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Set matplotlib plot style\n",
    "style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[100]['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.plot(df['time'],df['spread'], color='turquoise')\n",
    "ax.set(title='MRNA', ylabel='Price per Share') \n",
    "\n",
    "# ax.axvline(pd.to_datetime('2020-11-30'), color='slategray', lw=1.2, linestyle='--')\n",
    "# ax.text(pd.to_datetime('2020-04-12'), max(mrna['Adj Close']+5), 'Vaccine FDA EUA application', color='slategray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['return'] = df['spread'].pct_change().dropna() * 100\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.plot(df['time'],df['return'], color='lightcoral')\n",
    "ax.set(title='MRNA', ylabel='% Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we drop nans\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "daily_volatility = df[(df['return']<df['return'].max() )&(df['return']>df['return'].min())]['return'].std()\n",
    "print('Daily volatility: ', '{:.2f}%'.format(daily_volatility))\n",
    "\n",
    "monthly_trade_days = 21\n",
    "monthly_volatility = np.sqrt(monthly_trade_days) * daily_volatility\n",
    "print('Monthly volatility: ', '{:.2f}%'.format(monthly_volatility))\n",
    "\n",
    "yearly_trade_days = 252\n",
    "yearly_volatility = np.sqrt(yearly_trade_days) * daily_volatility\n",
    "print('Yearly volatility: ', '{:.2f}%'.format(yearly_volatility))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base_model constant_mean, Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = int(len(df) * 0.8)\n",
    "df_predict = pd.DataFrame()\n",
    "for i in range(split_size,len(df)-5):\n",
    "    df_train = df.iloc[i-500:i]\n",
    "    df_test = df.iloc[i:i+5]\n",
    "\n",
    "    basic_gm = arch_model(df_train['spread'], p=1, q=1, mean='constant', vol='GARCH', dist='normal') \n",
    "    gm_result = basic_gm.fit()\n",
    "    gm_forecast = gm_result.forecast(horizon = 5)\n",
    "    df_predict_tmp = gm_forecast.mean\n",
    "    for ii in range(5):\n",
    "        df_predict_tmp[f't.{ii+1}'] = df_test['spread'].values[ii]\n",
    "    df_predict_tmp['split_index'] = i \n",
    "    df_predict_tmp['time'] = df.iloc[i]['time']\n",
    "    df_predict = pd.concat([df_predict,df_predict_tmp])\n",
    "\n",
    "df_predict\n",
    "df_predict.to_csv('./data/df_predict_gmgarch.csv',sep='\\t',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict\n",
    "np.sqrt(np.mean((df_predict['h.1']-df_predict['t.1'])**2))\n",
    "# (df_predict['h.1']-df_predict['t.1'])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 5-period ahead forecast\n",
    "gm_forecast = gm_result.forecast(horizon = 5)\n",
    "\n",
    "# Print the forecast variance\n",
    "print(gm_forecast.variance[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(gm_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = gm_forecast.mean\n",
    "df_predict['t.1'] = 1\n",
    "df_predict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standardized residual\n",
    "gm_std_resid = gm_result.resid / gm_result.conditional_volatility\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.hist(gm_std_resid, color='salmon', bins=40)\n",
    "ax.set(title='Distribution of Standardized Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Constant mean, skewed t distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewt_gm = arch_model(df['spread'], p=1, q=1, mean='constant', vol='GARCH', dist='skewt') \n",
    "# skewt_result = skewt_gm.fit()\n",
    "\n",
    "\n",
    "split_size = int(len(df) * 0.8)\n",
    "df_predict = pd.DataFrame()\n",
    "for i in range(split_size,len(df)-5):\n",
    "    df_train = df.iloc[i-500:i]\n",
    "    df_test = df.iloc[i:i+5]\n",
    "\n",
    "    # basic_gm = arch_model(df_train['spread'], p=1, q=1, mean='constant', vol='GARCH', dist='normal') \n",
    "    skewt_gm = arch_model(df_train['spread'], p=1, q=1, mean='constant', vol='GARCH', dist='skewt') \n",
    "    skewt_result = skewt_gm.fit()\n",
    "    skewt_forecast = skewt_result.forecast(horizon = 5)\n",
    "    df_predict_tmp = skewt_forecast.mean\n",
    "    for ii in range(5):\n",
    "        df_predict_tmp[f't.{ii+1}'] = df_test['spread'].values[ii]\n",
    "    df_predict_tmp['split_index'] = i \n",
    "    df_predict_tmp['time'] = df.iloc[i]['time']\n",
    "    df_predict = pd.concat([df_predict,df_predict_tmp])\n",
    "\n",
    "df_predict\n",
    "df_predict.to_csv('./data/df_predict_skewt.csv',sep='\\t',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewt_result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model estimated volatility\n",
    "normal_volatility = gm_result.conditional_volatility\n",
    "skewt_volatility = skewt_result.conditional_volatility\n",
    "\n",
    "# Plot model fitting results\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(skewt_volatility, color = 'gold', label = 'Skewed-t Volatility')\n",
    "plt.plot(normal_volatility, color = 'turquoise', label = 'Normal Volatility')\n",
    "plt.plot(df['spread'], color = 'grey', label = 'Daily Returns', alpha = 0.4)\n",
    "plt.legend(loc = 'upper right', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4c. Autoregressive (AR) Mean, skewed t distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# armean_gm = arch_model(df['spread'], p=1, q=1, mean='AR', lags=1, vol='GARCH', dist='skewt') \n",
    "# armean_result = armean_gm.fit()\n",
    "\n",
    "\n",
    "for p in [1,2,3]:\n",
    "    for q in [1,2,3]:\n",
    "        split_size = int(len(df) * 0.8)\n",
    "        df_predict = pd.DataFrame()\n",
    "        for i in range(split_size,len(df)-5):\n",
    "            df_train = df.iloc[i-500:i]\n",
    "            df_test = df.iloc[i:i+5]\n",
    "\n",
    "            # basic_gm = arch_model(df_train['spread'], p=1, q=1, mean='constant', vol='GARCH', dist='normal') \n",
    "            # skewt_gm = arch_model(df_train['spread'], p=1, q=1, mean='constant', vol='GARCH', dist='skewt') \n",
    "            armean_gm = arch_model(df_train['spread'], p=p, q=q, mean='AR', lags=1, vol='GARCH', dist='skewt') \n",
    "            armean_result = armean_gm.fit()\n",
    "            armean_forecast = armean_result.forecast(horizon = 5)\n",
    "            df_predict_tmp = armean_forecast.mean\n",
    "            for ii in range(5):\n",
    "                df_predict_tmp[f't.{ii+1}'] = df_test['spread'].values[ii]\n",
    "            df_predict_tmp['split_index'] = i \n",
    "            df_predict_tmp['time'] = df.iloc[i]['time']\n",
    "            df_predict = pd.concat([df_predict,df_predict_tmp])\n",
    "\n",
    "        df_predict\n",
    "        df_predict.to_csv(f'./data/df_predict_armean_{p}_{q}.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "armean_result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "armean_volatility = armean_result.conditional_volatility.dropna() # uses lag, first row will be NaN\n",
    "skewt_volatility = skewt_volatility.iloc[1:] # drop first row\n",
    "\n",
    "# Plot model fitting results\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(skewt_volatility, color = 'gold', label = 'Constant Mean Volatility')\n",
    "plt.plot(armean_volatility, color = 'turquoise', label = 'AR Mean Volatility')\n",
    "plt.plot(df['spread'], color = 'grey', label = 'Daily Returns', alpha = 0.4)\n",
    "plt.legend(loc = 'upper right', frameon=False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation \n",
    "np.corrcoef(skewt_volatility, armean_volatility)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4d. EGARCH (asymmetric shock) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# egarch_gm = arch_model(df['spread'], p=1, q=1, o=1, mean='constant', vol='EGARCH', dist='skewt') \n",
    "# egarch_result = egarch_gm.fit()\n",
    "\n",
    "\n",
    "split_size = int(len(df) * 0.8)\n",
    "df_predict = pd.DataFrame()\n",
    "for i in range(split_size,len(df)-5):\n",
    "    df_train = df.iloc[i-500:i]\n",
    "    df_test = df.iloc[i:i+5]\n",
    "\n",
    "    # basic_gm = arch_model(df_train['spread'], p=1, q=1, mean='constant', vol='GARCH', dist='normal') \n",
    "    # skewt_gm = arch_model(df_train['spread'], p=1, q=1, mean='constant', vol='GARCH', dist='skewt') \n",
    "    # armean_gm = arch_model(df_train['spread'], p=1, q=1, mean='AR', lags=1, vol='GARCH', dist='skewt') \n",
    "    egarch_gm = arch_model(df_train['spread'], p=1, q=1, o=1, mean='constant', vol='EGARCH', dist='skewt') \n",
    "    egarch_result = egarch_gm.fit()\n",
    "    egarch_forecast = egarch_result.forecast(horizon = 1)\n",
    "    df_predict_tmp = egarch_forecast.mean\n",
    "    for ii in range(1):\n",
    "        df_predict_tmp[f't.{ii+1}'] = df_test['spread'].values[ii]\n",
    "    df_predict_tmp['split_index'] = i \n",
    "    df_predict_tmp['time'] = df.iloc[i]['time']\n",
    "    df_predict = pd.concat([df_predict,df_predict_tmp])\n",
    "\n",
    "df_predict\n",
    "df_predict.to_csv('./data/df_predict_egarch.csv',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egarch_result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4e. GJR-GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gjrgarch_gm = arch_model(df['spread'], p=1, q=1, o=1, mean='constant', vol='GARCH', dist='skewt') \n",
    "gjrgarch_result = gjrgarch_gm.fit()\n",
    "\n",
    "\n",
    "\n",
    "split_size = int(len(df) * 0.8)\n",
    "df_predict = pd.DataFrame()\n",
    "for i in range(split_size,len(df)-5):\n",
    "    df_train = df.iloc[i-500:i]\n",
    "    df_test = df.iloc[i:i+5]\n",
    "\n",
    "    # basic_gm = arch_model(df_train['spread'], p=1, q=1, mean='constant', vol='GARCH', dist='normal') \n",
    "    # skewt_gm = arch_model(df_train['spread'], p=1, q=1, mean='constant', vol='GARCH', dist='skewt') \n",
    "    # armean_gm = arch_model(df_train['spread'], p=1, q=1, mean='AR', lags=1, vol='GARCH', dist='skewt') \n",
    "    # egarch_gm = arch_model(df_train['spread'], p=1, q=1, o=1, mean='constant', vol='EGARCH', dist='skewt') \n",
    "    gjrgarch_gm = arch_model(df_train['spread'], p=1, q=1, o=1, mean='constant', vol='GARCH', dist='skewt') \n",
    "    gjrgarch_result = gjrgarch_gm.fit()\n",
    "    gjrgarch_forecast = gjrgarch_result.forecast(horizon = 5)\n",
    "    df_predict_tmp = gjrgarch_forecast.mean\n",
    "    for ii in range(5):\n",
    "        df_predict_tmp[f't.{ii+1}'] = df_test['spread'].values[ii]\n",
    "    df_predict_tmp['split_index'] = i \n",
    "    df_predict_tmp['time'] = df.iloc[i]['time']\n",
    "    df_predict = pd.concat([df_predict,df_predict_tmp])\n",
    "\n",
    "df_predict.to_csv('./data/df_predict_gjrgarch.csv',sep='\\t',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gjrgarch_result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gjrgarch_result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create dataframe of log likelihood, AIC, BIC for each model\n",
    "def goodness_of_fit():\n",
    "  global df\n",
    "  model_names = ['normal', 'skewt', 'GJR-GARCH', 'EGARCH']\n",
    "  models = [gm_result, skewt_result, gjrgarch_result, egarch_result]\n",
    "  likelihood = [model.loglikelihood for model in models]\n",
    "  aic = [model.aic for model in models]\n",
    "  bic = [model.bic for model in models]\n",
    "  dict = {'model':model_names, 'log likelihood':likelihood, 'aic':aic,'bic':bic}\n",
    "  df = pd.DataFrame(dict).set_index('model')\n",
    "  return df\n",
    "\n",
    "goodness_of_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight max log likelihood\n",
    "df.style.highlight_max(subset='log likelihood', color = 'yellow', axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight min AIC, BIC\n",
    "df.style.highlight_min(subset=['aic', 'bic'], color = 'yellow', axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameter stats from model summary\n",
    "parameters = pd.DataFrame({'parameter': gjrgarch_result.params,\n",
    "                           'p-value': gjrgarch_result.pvalues})\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standardized residual\n",
    "gjrgarch_std_resid = gjrgarch_result.resid / gjrgarch_result.conditional_volatility\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(gjrgarch_std_resid, color='lightcoral')\n",
    "ax.set(title='Standardized Resduals') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "for root, dirs, files in os.walk('./data'):\n",
    "    for file in files:\n",
    "        if file.startswith('df_predict'):\n",
    "            df = pd.read_csv(os.path.join(root, file),sep='\\t')\n",
    "            print(file,np.sqrt(np.mean((df['h.1']-df['t.1'])**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "import pandas as pd \n",
    "metric = Metrics()\n",
    "import os \n",
    "for root, dirs, files in os.walk('./data'):\n",
    "    for file in files:\n",
    "        if file.startswith('df_predict'):\n",
    "            df = pd.read_csv(os.path.join(root, file),sep='\\t')\n",
    "            print(file, metric.MetricsAll(df['t.1'],df['h.1']))\n",
    "            # print(file,np.sqrt(np.mean((df['h.1']-df['t.1'])**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "import pandas as pd \n",
    "metric = Metrics()\n",
    "import os \n",
    "for root, dirs, files in os.walk('./data'):\n",
    "    for file in files:\n",
    "        if file.startswith('lstm'):\n",
    "            df = pd.read_csv(os.path.join(root, file),sep=',')\n",
    "            df = df[df['true']!='true']\n",
    "            df['predict'] = df['predict'].astype(float)\n",
    "            df['true'] = df['true'].astype(float)\n",
    "            print(file, metric.MetricsAll(df['true'],df['predict']))\n",
    "            # print(file,np.sqrt(np.mean((df['h.1']-df['t.1'])**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "import pandas as pd \n",
    "metric = Metrics()\n",
    "import os \n",
    "for root, dirs, files in os.walk('./data'):\n",
    "    for file in files:\n",
    "        if file.startswith('fcnn'):\n",
    "            df = pd.read_csv(os.path.join(root, file),sep=',')\n",
    "            df = df[df['true']!='true']\n",
    "            df['predict'] = df['predict'].astype(float)\n",
    "            df['true'] = df['true'].astype(float)\n",
    "            print(file, metric.MetricsAll(df['true'],df['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (self.mae(y_true,y_pred), self.rmse(y_true,y_pred),self.wmape(y_true,y_pred),self.r_squared(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "import pandas as pd \n",
    "metric = Metrics()\n",
    "import os \n",
    "for root, dirs, files in os.walk('./data'):\n",
    "    for file in files:\n",
    "        if file.startswith('cnn'):\n",
    "            df = pd.read_csv(os.path.join(root, file),sep=',')\n",
    "            df = df[df['true']!='true']\n",
    "            df['predict'] = df['predict'].astype(float)\n",
    "            df['true'] = df['true'].astype(float)\n",
    "            print(file, metric.MetricsAll(df['true'],df['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "import pandas as pd \n",
    "metric = Metrics()\n",
    "import os \n",
    "for root, dirs, files in os.walk('./result'):\n",
    "    for file in files:\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(root, file),sep='\\t')\n",
    "        # print(df.head())\n",
    "        df = df[df['true']!='true']\n",
    "        df['predict'] = df['predict'].astype(float)\n",
    "        df['true'] = df['true'].astype(float)\n",
    "        print(file, metric.MetricsAll(df['true'],df['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 过滤garch模型到期日前两天的的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('./data/df_predict_armean.csv',sep='\\t')\n",
    "df_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('./result/df_trendawarelstm_到期日前两天.csv',sep='\\t')\n",
    "df_2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[df_1['split_index'].isin(df_2['split_index'].values + 100)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from metrics import *\n",
    "import pandas as pd \n",
    "metric = Metrics()\n",
    "import os \n",
    "for root, dirs, files in os.walk('./data'):\n",
    "    for file in files:\n",
    "        if file.startswith('df_predict'):\n",
    "            df = pd.read_csv(os.path.join(root, file),sep='\\t')\n",
    "            # print(file, metric.MetricsAll(df['t.1'],df['h.1']))\n",
    "            # print(file,np.sqrt(np.mean((df['h.1']-df['t.1'])**2)))\n",
    "\n",
    "            print(file, metric.MetricsAll(df[df['split_index'].isin(df_2['split_index'].values + 100)] ['t.1'],\n",
    "                                        df[df['split_index'].isin(df_2['split_index'].values + 100)] ['h.1']))\n",
    "            df[df['split_index'].isin(df_2['split_index'].values + 100)].to_csv(f'./result/{file}_到期日前两天.csv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['split_index'].isin(df_2['split_index'].values + 100)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhuqw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
